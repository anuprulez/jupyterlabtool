{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3e862af-95ec-4fc8-86f3-bb8d00dab261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sample_tf_code.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sample_tf_code.py\n",
    "\n",
    "#import tensorflow as tf\n",
    "\n",
    "def train_step(images, labels, mnist_model):\n",
    "  loss_history = []\n",
    "  with tf.GradientTape() as tape:\n",
    "    logits = mnist_model(images, training=True)\n",
    "\n",
    "    # Add asserts to check the shape of the output.\n",
    "    tf.debugging.assert_equal(logits.shape, (32, 10))\n",
    "\n",
    "    loss_value = loss_object(labels, logits)\n",
    "\n",
    "  loss_history.append(loss_value.numpy().mean())\n",
    "  grads = tape.gradient(loss_value, mnist_model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads, mnist_model.trainable_variables))\n",
    "  return np.mean(loss_history)\n",
    "\n",
    "\n",
    "def train(epochs, dataset):\n",
    "\n",
    "  mnist_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(16,[3,3], activation='relu', # convoutional layer\n",
    "                         input_shape=(None, None, 1)),\n",
    "    tf.keras.layers.Conv2D(16,[3,3], activation='relu'), # convoutional layer\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(10) # output layer\n",
    "  ])\n",
    "\n",
    "  optimizer = tf.keras.optimizers.Adam()\n",
    "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    \n",
    "  tot_loss = []\n",
    "  for epoch in range(epochs):\n",
    "    for (batch, (images, labels)) in enumerate(dataset):\n",
    "      b_loss = train_step(images, labels, mnist_model)\n",
    "      tot_loss.append(b_loss)\n",
    "    #print ('Epoch {} finished'.format(epoch))\n",
    "  return np.mean(tot_loss)\n",
    "\n",
    "(mnist_images, mnist_labels), _ = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((tf.cast(mnist_images[...,tf.newaxis]/255, tf.float32), tf.cast(mnist_labels,tf.int64)))\n",
    "\n",
    "dataset = dataset.shuffle(1000).batch(32)\n",
    "final_loss = train(1, dataset)\n",
    "print(final_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c43876f3-9e91-41b1-9377-6652038eecd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2814543\n",
      "{'annotation': None, 'contents_url': '/api/histories/7b55dbb89df8f4e5/contents', 'create_time': '2021-08-11T16:56:30.433598', 'deleted': False, 'empty': True, 'genome_build': None, 'id': '7b55dbb89df8f4e5', 'importable': False, 'model_class': 'History', 'name': '2814543', 'published': False, 'purged': False, 'size': 0, 'slug': None, 'state': 'new', 'state_details': {'discarded': 0, 'empty': 0, 'error': 0, 'failed_metadata': 0, 'new': 0, 'ok': 0, 'paused': 0, 'queued': 0, 'running': 0, 'setting_metadata': 0, 'upload': 0}, 'state_ids': {'discarded': [], 'empty': [], 'error': [], 'failed_metadata': [], 'new': [], 'ok': [], 'paused': [], 'queued': [], 'running': [], 'setting_metadata': [], 'upload': []}, 'tags': [], 'update_time': '2021-08-11T16:56:30.000437', 'url': '/api/histories/7b55dbb89df8f4e5', 'user_id': '2891970512fa2d5a', 'username_and_slug': None}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import bioblend\n",
    "from bioblend.galaxy import GalaxyInstance\n",
    "from bioblend.galaxy import histories\n",
    "from time import sleep\n",
    "\n",
    "server='http://127.0.0.1:9090/'\n",
    "key = '49aec9e3e881e3235c601147b8e353f2'\n",
    "file_name = \"sample_tf_code.py\"\n",
    "\n",
    "\n",
    "gi = GalaxyInstance(server, key=key)\n",
    "history = histories.HistoryClient(gi)\n",
    "\n",
    "rnd_int = random.randint(1, 10000000)\n",
    "\n",
    "print(rnd_int)\n",
    "\n",
    "new_history = history.create_history(str(rnd_int))\n",
    "print(new_history)\n",
    "uploaded_dataset = gi.tools.upload_file(file_name, new_history[\"id\"])\n",
    "sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3e553a9-df72-46ac-bae8-713c0ae90fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'implicit_collections': [],\n",
       " 'jobs': [{'create_time': '2021-08-11T16:56:30.692510',\n",
       "   'exit_code': None,\n",
       "   'galaxy_version': '21.05',\n",
       "   'history_id': '7b55dbb89df8f4e5',\n",
       "   'id': '683bc220e21425bb',\n",
       "   'model_class': 'Job',\n",
       "   'state': 'new',\n",
       "   'tool_id': 'upload1',\n",
       "   'update_time': '2021-08-11T16:56:30.712828'}],\n",
       " 'output_collections': [],\n",
       " 'outputs': [{'create_time': '2021-08-11T16:56:30.614628',\n",
       "   'data_type': 'galaxy.datatypes.data.Data',\n",
       "   'deleted': False,\n",
       "   'file_ext': 'auto',\n",
       "   'file_size': 0,\n",
       "   'genome_build': '?',\n",
       "   'hda_ldda': 'hda',\n",
       "   'hid': 1,\n",
       "   'history_content_type': 'dataset',\n",
       "   'history_id': '7b55dbb89df8f4e5',\n",
       "   'id': '683bc220e21425bb',\n",
       "   'metadata_dbkey': '?',\n",
       "   'misc_blurb': None,\n",
       "   'misc_info': None,\n",
       "   'model_class': 'HistoryDatasetAssociation',\n",
       "   'name': 'sample_tf_code.py',\n",
       "   'output_name': 'output0',\n",
       "   'peek': None,\n",
       "   'purged': False,\n",
       "   'state': 'queued',\n",
       "   'tags': [],\n",
       "   'update_time': '2021-08-11T16:56:30.673526',\n",
       "   'uuid': 'af50d31d-50f3-4568-9b83-5fccbab4058f',\n",
       "   'validated_state': 'unknown',\n",
       "   'validated_state_message': None,\n",
       "   'visible': True}],\n",
       " 'produces_entry_points': False}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uploaded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "373b4fb8-998a-4b18-ac79-4c8164a7db6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_name = \"run_jupyter_job\"\n",
    "hist_id = new_history[\"id\"]\n",
    "file_path = uploaded_dataset[\"outputs\"][0][\"id\"]\n",
    "tool_inputs = {\"inputs\": {\"select_file\": file_path}}\n",
    "tool_run = gi.tools.run_tool(hist_id, tool_name, tool_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e12031f-7cb2-4bc4-9fe5-bfd920a20fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'new'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_run[\"jobs\"][0][\"state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05055078-721b-4119-8c8f-4d83b317f184",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4667/335410567.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mfinal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4667/335410567.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, dataset)\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m       \u001b[0mb_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnist_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m       \u001b[0mtot_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m#print ('Epoch {} finished'.format(epoch))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4667/335410567.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(images, labels, mnist_model)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mloss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Add asserts to check the shape of the output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jltool/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1029\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1030\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jltool/lib/python3.9/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    378\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m  \u001b[0;31m# handle the corner case where self.layers is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jltool/lib/python3.9/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtensors\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mthan\u001b[0m \u001b[0mone\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m     \"\"\"\n\u001b[0;32m--> 420\u001b[0;31m     return self._run_internal_graph(\n\u001b[0m\u001b[1;32m    421\u001b[0m         inputs, training=training, mask=mask)\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jltool/lib/python3.9/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jltool/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1029\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1030\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jltool/lib/python3.9/site-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m               outputs, _apply_fn, inner_rank=self.rank + 1)\n\u001b[1;32m    266\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m           outputs = nn.bias_add(\n\u001b[0m\u001b[1;32m    268\u001b[0m               outputs, self.bias, data_format=self._tf_data_format)\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jltool/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jltool/lib/python3.9/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(value, bias, data_format, name)\u001b[0m\n\u001b[1;32m   3375\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3376\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3377\u001b[0;31m       return gen_nn_ops.bias_add(\n\u001b[0m\u001b[1;32m   3378\u001b[0m           value, bias, data_format=data_format, name=name)\n\u001b[1;32m   3379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jltool/lib/python3.9/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(value, bias, data_format, name)\u001b[0m\n\u001b[1;32m    672\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m    675\u001b[0m         _ctx, \"BiasAdd\", name, value, bias, \"data_format\", data_format)\n\u001b[1;32m    676\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def train_step(images, labels, mnist_model):\n",
    "  loss_history = []\n",
    "  with tf.GradientTape() as tape:\n",
    "    logits = mnist_model(images, training=True)\n",
    "\n",
    "    # Add asserts to check the shape of the output.\n",
    "    tf.debugging.assert_equal(logits.shape, (32, 10))\n",
    "\n",
    "    loss_value = loss_object(labels, logits)\n",
    "\n",
    "  loss_history.append(loss_value.numpy().mean())\n",
    "  grads = tape.gradient(loss_value, mnist_model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads, mnist_model.trainable_variables))\n",
    "  return np.mean(loss_history)\n",
    "\n",
    "\n",
    "def train(epochs, dataset):\n",
    "      \n",
    "  (mnist_images, mnist_labels), _ = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "  dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (tf.cast(mnist_images[...,tf.newaxis]/255, tf.float32),\n",
    "     tf.cast(mnist_labels,tf.int64)))\n",
    "\n",
    "  dataset = dataset.shuffle(1000).batch(32)\n",
    "\n",
    "  mnist_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(16,[3,3], activation='relu', # convoutional layer\n",
    "                         input_shape=(None, None, 1)),\n",
    "    tf.keras.layers.Conv2D(16,[3,3], activation='relu'), # convoutional layer\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(10) # output layer\n",
    "  ])\n",
    "\n",
    "  optimizer = tf.keras.optimizers.Adam()\n",
    "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    \n",
    "  tot_loss = []\n",
    "  for epoch in range(epochs):\n",
    "    for (batch, (images, labels)) in enumerate(dataset):\n",
    "      b_loss = train_step(images, labels, mnist_model)\n",
    "      tot_loss.append(b_loss)\n",
    "    #print ('Epoch {} finished'.format(epoch))\n",
    "  return np.mean(tot_loss)\n",
    "    \n",
    "final_loss = train(1, dataset)\n",
    "print(final_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf161c7d-d0d4-4b1f-ab35-84b5646ed235",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
