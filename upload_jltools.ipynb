{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f3e862af-95ec-4fc8-86f3-bb8d00dab261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sample_tf_code.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sample_tf_code.py\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def train_step(images, labels, mnist_model, optimizer, loss_object):\n",
    "    loss_history = []\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = mnist_model(images, training=True)\n",
    "        tf.debugging.assert_equal(logits.shape, (32, 10))\n",
    "        loss_value = loss_object(labels, logits)\n",
    "    loss_history.append(loss_value.numpy().mean())\n",
    "    grads = tape.gradient(loss_value, mnist_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, mnist_model.trainable_variables))\n",
    "    return np.mean(loss_history), mnist_model\n",
    "\n",
    "\n",
    "def train(epochs, dataset, train_step):\n",
    "\n",
    "    tot_loss = []\n",
    "    weights = None\n",
    "\n",
    "    mnist_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(16,[3,3], activation='relu',\n",
    "                         input_shape=(None, None, 1)),\n",
    "        tf.keras.layers.Conv2D(16,[3,3], activation='relu'),\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(10)\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "  \n",
    "    for epoch in range(epochs):\n",
    "        for (batch, (images, labels)) in enumerate(dataset):\n",
    "            b_loss, mnist_model = train_step(images, labels, mnist_model, optimizer, loss_object)\n",
    "            tot_loss.append(b_loss)\n",
    "    weights = mnist_model.get_weights()\n",
    "    return np.mean(tot_loss), weights\n",
    "\n",
    "\n",
    "def rev_value():\n",
    "    (mnist_images, mnist_labels), _ = tf.keras.datasets.mnist.load_data()\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((tf.cast(mnist_images[...,tf.newaxis]/255, tf.float32), tf.cast(mnist_labels,tf.int64)))\n",
    "    dataset = dataset.shuffle(1000).batch(32)\n",
    "    return train(1, dataset, train_step)\n",
    "\n",
    "loss, weights = rev_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c43876f3-9e91-41b1-9377-6652038eecd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from time import sleep\n",
    "import bioblend\n",
    "from bioblend.galaxy import GalaxyInstance\n",
    "from bioblend.galaxy import histories\n",
    "\n",
    "\n",
    "server = 'http://127.0.0.1:9090/'\n",
    "key = '49aec9e3e881e3235c601147b8e353f2'\n",
    "file_path = \"sample_tf_code.py\"\n",
    "tool_name = \"run_jupyter_job\"\n",
    "\n",
    "\n",
    "def run_dynamic_code(server, key, file_path, tool_name=\"run_jupyter_job\"):\n",
    "    gi = GalaxyInstance(server, key=key)\n",
    "    history = histories.HistoryClient(gi)\n",
    "    new_history = history.create_history(str(random.randint(1, 10000000)))\n",
    "    uploaded_dataset = gi.tools.upload_file(file_name, new_history[\"id\"])\n",
    "    sleep(20)\n",
    "    hist_id = new_history[\"id\"]\n",
    "    uploaded_file_path = uploaded_dataset[\"outputs\"][0][\"id\"]\n",
    "    tool_run = gi.tools.run_tool(hist_id, tool_name, {\"inputs\": {\"select_file\": uploaded_file_path}})\n",
    "    print(tool_run[\"jobs\"][0][\"state\"])\n",
    "run_dynamic_code(server, key, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eb80e1-ce5b-4384-864b-ec32b7a8847e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9a7f94-80cd-4092-aebf-b5799c190488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e553a9-df72-46ac-bae8-713c0ae90fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373b4fb8-998a-4b18-ac79-4c8164a7db6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e12031f-7cb2-4bc4-9fe5-bfd920a20fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05055078-721b-4119-8c8f-4d83b317f184",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf161c7d-d0d4-4b1f-ab35-84b5646ed235",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
