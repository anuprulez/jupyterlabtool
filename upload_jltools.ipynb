{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3e862af-95ec-4fc8-86f3-bb8d00dab261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sample_tf_code.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sample_tf_code.py\n",
    "\n",
    "#import tensorflow as tf\n",
    "\n",
    "(mnist_images, mnist_labels), _ = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "  (tf.cast(mnist_images[...,tf.newaxis]/255, tf.float32),\n",
    "   tf.cast(mnist_labels,tf.int64)))\n",
    "\n",
    "dataset = dataset.shuffle(1000).batch(32)\n",
    "\n",
    "mnist_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Conv2D(16,[3,3], activation='relu', # convoutional layer\n",
    "                         input_shape=(None, None, 1)),\n",
    "  tf.keras.layers.Conv2D(16,[3,3], activation='relu'), # convoutional layer\n",
    "  tf.keras.layers.GlobalAveragePooling2D(),\n",
    "  tf.keras.layers.Dense(10) # output layer\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "\n",
    "def train_step(images, labels):\n",
    "  with tf.GradientTape() as tape:\n",
    "    logits = mnist_model(images, training=True)\n",
    "\n",
    "    # Add asserts to check the shape of the output.\n",
    "    tf.debugging.assert_equal(logits.shape, (32, 10))\n",
    "\n",
    "    loss_value = loss_object(labels, logits)\n",
    "\n",
    "  loss_history.append(loss_value.numpy().mean())\n",
    "  grads = tape.gradient(loss_value, mnist_model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads, mnist_model.trainable_variables))\n",
    "  return np.mean(loss_history)\n",
    "\n",
    "\n",
    "def train(epochs):\n",
    "  tot_loss = []\n",
    "  for epoch in range(epochs):\n",
    "    for (batch, (images, labels)) in enumerate(dataset):\n",
    "      b_loss = train_step(images, labels)\n",
    "      tot_loss.append(b_loss)\n",
    "    #print ('Epoch {} finished'.format(epoch))\n",
    "  return np.mean(tot_loss)\n",
    "    \n",
    "final_loss = train(epochs = 1)\n",
    "print(final_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c43876f3-9e91-41b1-9377-6652038eecd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8674487\n",
      "{'annotation': None, 'contents_url': '/api/histories/54f2a3a23292eb07/contents', 'create_time': '2021-08-11T16:37:16.638814', 'deleted': False, 'empty': True, 'genome_build': None, 'id': '54f2a3a23292eb07', 'importable': False, 'model_class': 'History', 'name': '8674487', 'published': False, 'purged': False, 'size': 0, 'slug': None, 'state': 'new', 'state_details': {'discarded': 0, 'empty': 0, 'error': 0, 'failed_metadata': 0, 'new': 0, 'ok': 0, 'paused': 0, 'queued': 0, 'running': 0, 'setting_metadata': 0, 'upload': 0}, 'state_ids': {'discarded': [], 'empty': [], 'error': [], 'failed_metadata': [], 'new': [], 'ok': [], 'paused': [], 'queued': [], 'running': [], 'setting_metadata': [], 'upload': []}, 'tags': [], 'update_time': '2021-08-11T16:37:16.000644', 'url': '/api/histories/54f2a3a23292eb07', 'user_id': '2891970512fa2d5a', 'username_and_slug': None}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import bioblend\n",
    "from bioblend.galaxy import GalaxyInstance\n",
    "from bioblend.galaxy import histories\n",
    "from time import sleep\n",
    "\n",
    "server='http://127.0.0.1:9090/'\n",
    "key = '49aec9e3e881e3235c601147b8e353f2'\n",
    "file_name = \"sample_tf_code.py\"\n",
    "\n",
    "\n",
    "gi = GalaxyInstance(server, key=key)\n",
    "history = histories.HistoryClient(gi)\n",
    "\n",
    "rnd_int = random.randint(1, 10000000)\n",
    "\n",
    "print(rnd_int)\n",
    "\n",
    "new_history = history.create_history(str(rnd_int))\n",
    "print(new_history)\n",
    "uploaded_dataset = gi.tools.upload_file(file_name, new_history[\"id\"])\n",
    "sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3e553a9-df72-46ac-bae8-713c0ae90fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'implicit_collections': [],\n",
       " 'jobs': [{'create_time': '2021-08-11T16:37:16.933976',\n",
       "   'exit_code': None,\n",
       "   'galaxy_version': '21.05',\n",
       "   'history_id': '54f2a3a23292eb07',\n",
       "   'id': '54f2a3a23292eb07',\n",
       "   'model_class': 'Job',\n",
       "   'state': 'new',\n",
       "   'tool_id': 'upload1',\n",
       "   'update_time': '2021-08-11T16:37:16.954368'}],\n",
       " 'output_collections': [],\n",
       " 'outputs': [{'create_time': '2021-08-11T16:37:16.862314',\n",
       "   'data_type': 'galaxy.datatypes.data.Data',\n",
       "   'deleted': False,\n",
       "   'file_ext': 'auto',\n",
       "   'file_size': 0,\n",
       "   'genome_build': '?',\n",
       "   'hda_ldda': 'hda',\n",
       "   'hid': 1,\n",
       "   'history_content_type': 'dataset',\n",
       "   'history_id': '54f2a3a23292eb07',\n",
       "   'id': '54f2a3a23292eb07',\n",
       "   'metadata_dbkey': '?',\n",
       "   'misc_blurb': None,\n",
       "   'misc_info': None,\n",
       "   'model_class': 'HistoryDatasetAssociation',\n",
       "   'name': 'sample_tf_code.py',\n",
       "   'output_name': 'output0',\n",
       "   'peek': None,\n",
       "   'purged': False,\n",
       "   'state': 'queued',\n",
       "   'tags': [],\n",
       "   'update_time': '2021-08-11T16:37:16.914081',\n",
       "   'uuid': 'a515a7f4-5024-4210-9042-ca449bb66779',\n",
       "   'validated_state': 'unknown',\n",
       "   'validated_state_message': None,\n",
       "   'visible': True}],\n",
       " 'produces_entry_points': False}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uploaded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "373b4fb8-998a-4b18-ac79-4c8164a7db6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_name = \"run_jupyter_job\"\n",
    "hist_id = new_history[\"id\"]\n",
    "file_path = uploaded_dataset[\"outputs\"][0][\"id\"]\n",
    "tool_inputs = {\"inputs\": {\"select_file\": file_path}}\n",
    "tool_run = gi.tools.run_tool(hist_id, tool_name, tool_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e12031f-7cb2-4bc4-9fe5-bfd920a20fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'new'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_run[\"jobs\"][0][\"state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05055078-721b-4119-8c8f-4d83b317f184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits:  [[ 0.02170621 -0.00361767 -0.04694578 -0.02845725 -0.04981812  0.02062837\n",
      "  -0.04882843 -0.0838387   0.05537724 -0.0555088 ]]\n",
      "2.0063226\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "(mnist_images, mnist_labels), _ = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "  (tf.cast(mnist_images[...,tf.newaxis]/255, tf.float32),\n",
    "   tf.cast(mnist_labels,tf.int64)))\n",
    "\n",
    "dataset = dataset.shuffle(1000).batch(32)\n",
    "\n",
    "mnist_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Conv2D(16,[3,3], activation='relu', # convoutional layer\n",
    "                         input_shape=(None, None, 1)),\n",
    "  tf.keras.layers.Conv2D(16,[3,3], activation='relu'), # convoutional layer\n",
    "  tf.keras.layers.GlobalAveragePooling2D(),\n",
    "  tf.keras.layers.Dense(10) # output layer\n",
    "])\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "\n",
    "def train_step(images, labels):\n",
    "  with tf.GradientTape() as tape:\n",
    "    logits = mnist_model(images, training=True)\n",
    "\n",
    "    # Add asserts to check the shape of the output.\n",
    "    tf.debugging.assert_equal(logits.shape, (32, 10))\n",
    "\n",
    "    loss_value = loss_object(labels, logits)\n",
    "\n",
    "  loss_history.append(loss_value.numpy().mean())\n",
    "  grads = tape.gradient(loss_value, mnist_model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads, mnist_model.trainable_variables))\n",
    "  return np.mean(loss_history)\n",
    "\n",
    "\n",
    "def train(epochs):\n",
    "  tot_loss = []\n",
    "  for epoch in range(epochs):\n",
    "    for (batch, (images, labels)) in enumerate(dataset):\n",
    "      b_loss = train_step(images, labels)\n",
    "      tot_loss.append(b_loss)\n",
    "    #print ('Epoch {} finished'.format(epoch))\n",
    "  return np.mean(tot_loss)\n",
    "    \n",
    "final_loss = train(epochs = 1)\n",
    "print(final_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf161c7d-d0d4-4b1f-ab35-84b5646ed235",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
